{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020147,
     "end_time": "2020-10-01T00:22:50.483942",
     "exception": false,
     "start_time": "2020-10-01T00:22:50.463795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Now that you know how to access and examine a dataset, you're ready to write your first SQL query!  As you'll soon see, SQL queries will help you sort through a massive dataset, to retrieve only the information that you need.  \n",
    "\n",
    "We'll begin by using the keywords **SELECT**, **FROM**, and **WHERE** to get data from specific columns based on conditions you specify. \n",
    "\n",
    "For clarity, we'll work with a small imaginary dataset `pet_records` which contains just one table, called `pets`. \n",
    "\n",
    "![](https://i.imgur.com/fI5Pvvp.png)\n",
    "\n",
    "# SELECT ... FROM\n",
    "\n",
    "The most basic SQL query selects a single column from a single table.  To do this, \n",
    "- specify the column you want after the word **SELECT**, and then \n",
    "- specify the table after the word **FROM**.  \n",
    "\n",
    "For instance, to select the `Name` column (from the `pets` table in the `pet_records` database in the `bigquery-public-data` project), our query would appear as follows:  \n",
    "\n",
    "![](https://i.imgur.com/c3GxYRt.png)\n",
    "\n",
    "Note that when writing an SQL query, the argument we pass to **FROM** is *not* in single or double quotation marks (' or \"). It is in backticks (\\`).\n",
    "\n",
    "# WHERE ...\n",
    "\n",
    "BigQuery datasets are large, so you'll usually want to return only the rows meeting specific conditions. You can do this using the **WHERE** clause.\n",
    "\n",
    "The query below returns the entries from the `Name` column that are in rows where the `Animal` column has the text `'Cat'`. \n",
    "\n",
    "![](https://i.imgur.com/HJOT8Kb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018301,
     "end_time": "2020-10-01T00:22:50.522248",
     "exception": false,
     "start_time": "2020-10-01T00:22:50.503947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example: What are all the U.S. cities in the OpenAQ dataset?\n",
    "\n",
    "Now that you've got the basics down, let's work through an example with a real dataset. We'll use an [OpenAQ](https://openaq.org) dataset about air quality.\n",
    "\n",
    "First, we'll set up everything we need to run queries and take a quick peek at what tables are in our database.  (_Since you learned how to do this in the previous tutorial, we have hidden the code.  But if you'd like to take a peek, you need only click on the \"Code\" button below._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:50.577035Z",
     "iopub.status.busy": "2020-10-01T00:22:50.576194Z",
     "iopub.status.idle": "2020-10-01T00:22:51.183445Z",
     "shell.execute_reply": "2020-10-01T00:22:51.182513Z"
    },
    "papermill": {
     "duration": 0.638453,
     "end_time": "2020-10-01T00:22:51.183624",
     "exception": false,
     "start_time": "2020-10-01T00:22:50.545171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kaggle's public dataset BigQuery integration.\n",
      "global_air_quality\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"openaq\" dataset\n",
    "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# List all the tables in the \"openaq\" dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# Print names of all tables in the dataset (there's only one!)\n",
    "for table in tables:  \n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019844,
     "end_time": "2020-10-01T00:22:51.226701",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.206857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The dataset contains only one table, called `global_air_quality`.  We'll fetch the table and take a peek at the first few rows to see what sort of data it contains.  (_Again, we have hidden the code.  To take a peek, click on the \"Code\" button below._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:51.279711Z",
     "iopub.status.busy": "2020-10-01T00:22:51.278903Z",
     "iopub.status.idle": "2020-10-01T00:22:51.838063Z",
     "shell.execute_reply": "2020-10-01T00:22:51.837274Z"
    },
    "papermill": {
     "duration": 0.591295,
     "end_time": "2020-10-01T00:22:51.838214",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.246919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>pollutant</th>\n",
       "      <th>value</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>source_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>averaged_over_in_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTM Layout, Bengaluru - KSPCB</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IN</td>\n",
       "      <td>co</td>\n",
       "      <td>910.00</td>\n",
       "      <td>2018-02-22 03:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>CPCB</td>\n",
       "      <td>12.912811</td>\n",
       "      <td>77.60922</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTM Layout, Bengaluru - KSPCB</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IN</td>\n",
       "      <td>no2</td>\n",
       "      <td>131.87</td>\n",
       "      <td>2018-02-22 03:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>CPCB</td>\n",
       "      <td>12.912811</td>\n",
       "      <td>77.60922</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BTM Layout, Bengaluru - KSPCB</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IN</td>\n",
       "      <td>o3</td>\n",
       "      <td>15.57</td>\n",
       "      <td>2018-02-22 03:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>CPCB</td>\n",
       "      <td>12.912811</td>\n",
       "      <td>77.60922</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BTM Layout, Bengaluru - KSPCB</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IN</td>\n",
       "      <td>pm25</td>\n",
       "      <td>45.62</td>\n",
       "      <td>2018-02-22 03:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>CPCB</td>\n",
       "      <td>12.912811</td>\n",
       "      <td>77.60922</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BTM Layout, Bengaluru - KSPCB</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>IN</td>\n",
       "      <td>so2</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2018-02-22 03:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>CPCB</td>\n",
       "      <td>12.912811</td>\n",
       "      <td>77.60922</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        location       city country pollutant   value  \\\n",
       "0  BTM Layout, Bengaluru - KSPCB  Bengaluru      IN        co  910.00   \n",
       "1  BTM Layout, Bengaluru - KSPCB  Bengaluru      IN       no2  131.87   \n",
       "2  BTM Layout, Bengaluru - KSPCB  Bengaluru      IN        o3   15.57   \n",
       "3  BTM Layout, Bengaluru - KSPCB  Bengaluru      IN      pm25   45.62   \n",
       "4  BTM Layout, Bengaluru - KSPCB  Bengaluru      IN       so2    4.49   \n",
       "\n",
       "                  timestamp   unit source_name   latitude  longitude  \\\n",
       "0 2018-02-22 03:00:00+00:00  µg/m³        CPCB  12.912811   77.60922   \n",
       "1 2018-02-22 03:00:00+00:00  µg/m³        CPCB  12.912811   77.60922   \n",
       "2 2018-02-22 03:00:00+00:00  µg/m³        CPCB  12.912811   77.60922   \n",
       "3 2018-02-22 03:00:00+00:00  µg/m³        CPCB  12.912811   77.60922   \n",
       "4 2018-02-22 03:00:00+00:00  µg/m³        CPCB  12.912811   77.60922   \n",
       "\n",
       "   averaged_over_in_hours  \n",
       "0                    0.25  \n",
       "1                    0.25  \n",
       "2                    0.25  \n",
       "3                    0.25  \n",
       "4                    0.25  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Construct a reference to the \"global_air_quality\" table\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"global_air_quality\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020277,
     "end_time": "2020-10-01T00:22:51.879399",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.859122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Everything looks good! So, let's put together a query. Say we want to select all the values from the `city` column that are in rows where the `country` column is `'US'` (for \"United States\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:51.927313Z",
     "iopub.status.busy": "2020-10-01T00:22:51.926523Z",
     "iopub.status.idle": "2020-10-01T00:22:51.930134Z",
     "shell.execute_reply": "2020-10-01T00:22:51.929491Z"
    },
    "papermill": {
     "duration": 0.030367,
     "end_time": "2020-10-01T00:22:51.930273",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.899906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
    "query = \"\"\"\n",
    "        SELECT city\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019929,
     "end_time": "2020-10-01T00:22:51.970902",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.950973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Take the time now to ensure that this query lines up with what you learned above.  \n",
    "\n",
    "# Submitting the query to the dataset\n",
    "\n",
    "We're ready to use this query to get information from the OpenAQ dataset.  As in the previous tutorial, the first step is to create a [`Client`](https://google-cloud.readthedocs.io/en/latest/bigquery/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:52.018860Z",
     "iopub.status.busy": "2020-10-01T00:22:52.017886Z",
     "iopub.status.idle": "2020-10-01T00:22:52.023526Z",
     "shell.execute_reply": "2020-10-01T00:22:52.022907Z"
    },
    "papermill": {
     "duration": 0.03164,
     "end_time": "2020-10-01T00:22:52.023658",
     "exception": false,
     "start_time": "2020-10-01T00:22:51.992018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kaggle's public dataset BigQuery integration.\n"
     ]
    }
   ],
   "source": [
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020402,
     "end_time": "2020-10-01T00:22:52.064786",
     "exception": false,
     "start_time": "2020-10-01T00:22:52.044384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We begin by setting up the query with the [`query()`](https://google-cloud.readthedocs.io/en/latest/bigquery/generated/google.cloud.bigquery.client.Client.query.html#google.cloud.bigquery.client.Client.query) method.  We run the method with the default parameters, but this method also allows us to specify more complicated settings that you can read about in [the documentation](https://google-cloud.readthedocs.io/en/latest/bigquery/generated/google.cloud.bigquery.client.Client.query.html#google.cloud.bigquery.client.Client.query).  We'll revisit this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:52.114636Z",
     "iopub.status.busy": "2020-10-01T00:22:52.113262Z",
     "iopub.status.idle": "2020-10-01T00:22:53.709508Z",
     "shell.execute_reply": "2020-10-01T00:22:53.708570Z"
    },
    "papermill": {
     "duration": 1.623949,
     "end_time": "2020-10-01T00:22:53.709670",
     "exception": false,
     "start_time": "2020-10-01T00:22:52.085721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the query\n",
    "query_job = client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021866,
     "end_time": "2020-10-01T00:22:53.753625",
     "exception": false,
     "start_time": "2020-10-01T00:22:53.731759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we run the query and convert the results to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:53.805794Z",
     "iopub.status.busy": "2020-10-01T00:22:53.804980Z",
     "iopub.status.idle": "2020-10-01T00:22:54.498453Z",
     "shell.execute_reply": "2020-10-01T00:22:54.497585Z"
    },
    "papermill": {
     "duration": 0.723007,
     "end_time": "2020-10-01T00:22:54.498605",
     "exception": false,
     "start_time": "2020-10-01T00:22:53.775598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API request - run the query, and return a pandas DataFrame\n",
    "us_cities = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020584,
     "end_time": "2020-10-01T00:22:54.540632",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.520048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we've got a pandas DataFrame called `us_cities`, which we can use like any other DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:54.597769Z",
     "iopub.status.busy": "2020-10-01T00:22:54.594432Z",
     "iopub.status.idle": "2020-10-01T00:22:54.603328Z",
     "shell.execute_reply": "2020-10-01T00:22:54.602691Z"
    },
    "papermill": {
     "duration": 0.041709,
     "end_time": "2020-10-01T00:22:54.603476",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.561767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phoenix-Mesa-Scottsdale                     88\n",
       "Houston                                     82\n",
       "Los Angeles-Long Beach-Santa Ana            68\n",
       "Riverside-San Bernardino-Ontario            60\n",
       "New York-Northern New Jersey-Long Island    60\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What five cities have the most measurements?\n",
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021268,
     "end_time": "2020-10-01T00:22:54.646491",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.625223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# More queries\n",
    "\n",
    "If you want multiple columns, you can select them with a comma between the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:54.696286Z",
     "iopub.status.busy": "2020-10-01T00:22:54.695372Z",
     "iopub.status.idle": "2020-10-01T00:22:54.699583Z",
     "shell.execute_reply": "2020-10-01T00:22:54.698903Z"
    },
    "papermill": {
     "duration": 0.031445,
     "end_time": "2020-10-01T00:22:54.699726",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.668281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022173,
     "end_time": "2020-10-01T00:22:54.744243",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.722070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can select all columns with a `*` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:54.795574Z",
     "iopub.status.busy": "2020-10-01T00:22:54.794418Z",
     "iopub.status.idle": "2020-10-01T00:22:54.798323Z",
     "shell.execute_reply": "2020-10-01T00:22:54.797667Z"
    },
    "papermill": {
     "duration": 0.032044,
     "end_time": "2020-10-01T00:22:54.798488",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.766444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022033,
     "end_time": "2020-10-01T00:22:54.842911",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.820878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Q&A: Notes on formatting\n",
    "\n",
    "The formatting of the SQL query might feel unfamiliar. If you have any questions, you can ask in the comments section at the bottom of this page.  Here are answers to two common questions:\n",
    "\n",
    "### **Question: What's up with the triple quotation marks (\"\"\")?**\n",
    "\n",
    "_Answer_: These tell Python that everything inside them is a single string, even though we have line breaks in it. The line breaks aren't necessary, but they make it easier to read your query.\n",
    "\n",
    "### **Question: Do you need to capitalize SELECT and FROM?**\n",
    "\n",
    "_Answer_: No, SQL doesn't care about capitalization. However, it's customary to capitalize your SQL commands, and it makes your queries a bit easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022032,
     "end_time": "2020-10-01T00:22:54.887410",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.865378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Working with big datasets\n",
    "\n",
    "BigQuery datasets can be huge. We allow you to do a lot of computation for free, but everyone has some limit.\n",
    "\n",
    "**Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset.**\n",
    "\n",
    "The [biggest dataset currently on Kaggle](https://www.kaggle.com/github/github-repos) is 3TB, so you can go through your 30-day limit in a couple queries if you aren't careful.\n",
    "\n",
    "Don't worry though: we'll teach you how to avoid scanning too much data at once, so that you don't run over your limit.\n",
    "\n",
    "To begin,you can estimate the size of any query before running it. Here is an example using the (*very large!*) Hacker News dataset. To see how much data a query will scan, we create a `QueryJobConfig` object and set the `dry_run` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:54.942441Z",
     "iopub.status.busy": "2020-10-01T00:22:54.941656Z",
     "iopub.status.idle": "2020-10-01T00:22:55.280581Z",
     "shell.execute_reply": "2020-10-01T00:22:55.279885Z"
    },
    "papermill": {
     "duration": 0.371134,
     "end_time": "2020-10-01T00:22:55.280727",
     "exception": false,
     "start_time": "2020-10-01T00:22:54.909593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This query will process 429036722 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\"\n",
    "        SELECT score, title\n",
    "        FROM `bigquery-public-data.hacker_news.full`\n",
    "        WHERE type = \"job\" \n",
    "        \"\"\"\n",
    "\n",
    "# Create a QueryJobConfig object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "# API request - dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022414,
     "end_time": "2020-10-01T00:22:55.326123",
     "exception": false,
     "start_time": "2020-10-01T00:22:55.303709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:55.381783Z",
     "iopub.status.busy": "2020-10-01T00:22:55.380805Z",
     "iopub.status.idle": "2020-10-01T00:22:55.930674Z",
     "shell.execute_reply": "2020-10-01T00:22:55.931691Z"
    },
    "papermill": {
     "duration": 0.583013,
     "end_time": "2020-10-01T00:22:55.931913",
     "exception": false,
     "start_time": "2020-10-01T00:22:55.348900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 Query exceeded limit for bytes billed: 1000000. 429916160 or higher required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ffb41147547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# API request - try to run the query, and return a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msafe_query_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mimported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \"\"\"\n\u001b[0;32m-> 2927\u001b[0;31m         return self.result().to_dataframe(\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2858\u001b[0m         \"\"\"\n\u001b[0;32m-> 2859\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2860\u001b[0m         \u001b[0;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 Query exceeded limit for bytes billed: 1000000. 429916160 or higher required."
     ]
    }
   ],
   "source": [
    "# Only run the query if it's less than 1 MB\n",
    "ONE_MB = 1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 MB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "safe_query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024365,
     "end_time": "2020-10-01T00:22:55.980654",
     "exception": false,
     "start_time": "2020-10-01T00:22:55.956289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this case, the query was cancelled, because the limit of 1 MB was exceeded.  However, we can increase the limit to run the query successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:22:56.038648Z",
     "iopub.status.busy": "2020-10-01T00:22:56.037862Z",
     "iopub.status.idle": "2020-10-01T00:23:00.517644Z",
     "shell.execute_reply": "2020-10-01T00:23:00.516997Z"
    },
    "papermill": {
     "duration": 4.511874,
     "end_time": "2020-10-01T00:23:00.517796",
     "exception": false,
     "start_time": "2020-10-01T00:22:56.005922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.874250078939059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run the query if it's less than 1 GB\n",
    "ONE_GB = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 GB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "# Print average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023699,
     "end_time": "2020-10-01T00:23:00.573559",
     "exception": false,
     "start_time": "2020-10-01T00:23:00.549860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your turn\n",
    "\n",
    "Writing **SELECT** statements is the key to using SQL. So **[try your new skills](https://www.kaggle.com/kernels/fork/681989)**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023739,
     "end_time": "2020-10-01T00:23:00.621428",
     "exception": false,
     "start_time": "2020-10-01T00:23:00.597689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161314) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 15.747857,
   "end_time": "2020-10-01T00:23:00.755309",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-01T00:22:45.007452",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
